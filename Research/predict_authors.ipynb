{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict_authors.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1omz7RivbObFIVOsmXCRGUWTcR6aPesMa","authorship_tag":"ABX9TyP+GzWfsKngQlhyxyFsuFhr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"rDQQSrwCg47r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606462982084,"user_tz":-60,"elapsed":67567,"user":{"displayName":"nagaraj Asundi","photoUrl":"","userId":"14146095082475060735"}},"outputId":"0baca9fc-ddf6-4ff4-bf67-22b81e05186b"},"source":["!pip install chars2vec\n","!pip install -U sentence-transformers\n","\n","from sentence_transformers import SentenceTransformer\n","import chars2vec\n","import pickle\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import class_weight\n","import json\n","\n","chars2vec_model = chars2vec.load_model('data/Auth2Vec_model')\n","bert_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n","\n","with open('data/index2auth.pickle','rb') as file:\n","  index2auth=pickle.load(file)\n","\n","with open('data/venue_fullforms','rb') as file:\n","  venue_fullforms=pickle.load(file) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting chars2vec\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/0a/8c327aae23e0532d239ec7b30446aca765eb5d9547b4c4b09cdd82e49797/chars2vec-0.1.7.tar.gz (8.1MB)\n","\u001b[K     |████████████████████████████████| 8.1MB 4.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: chars2vec\n","  Building wheel for chars2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chars2vec: filename=chars2vec-0.1.7-cp36-none-any.whl size=8111096 sha256=4219de1189189b9e0edcb5c3c82155a5c8d5ca503cd8621513a69f8664310180\n","  Stored in directory: /root/.cache/pip/wheels/97/b6/65/d7e778ef1213ec77d315aea0f536068b96e36cc94c02abbfde\n","Successfully built chars2vec\n","Installing collected packages: chars2vec\n","Successfully installed chars2vec-0.1.7\n","Collecting sentence-transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/5a/6e41e8383913dd2ba923cdcd02be2e03911595f4d2f9de559ecbed80d2d3/sentence-transformers-0.3.9.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 3.1MB/s \n","\u001b[?25hCollecting transformers<3.6.0,>=3.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 7.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n","Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n","Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (20.4)\n","Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (0.8)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 39.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied, skipping upgrade: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (3.12.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 40.8MB/s \n","\u001b[?25hCollecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 41.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.6.0,>=3.1.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.17.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers<3.6.0,>=3.1.0->sentence-transformers) (50.3.2)\n","Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.6.0,>=3.1.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2020.11.8)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.6.0,>=3.1.0->sentence-transformers) (1.24.3)\n","Building wheels for collected packages: sentence-transformers, sacremoses\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.9-cp36-none-any.whl size=101036 sha256=170b4fe4dedc70fc3ca1bc150efef2e902e92db8e7f94080e3cb33bc3a081b65\n","  Stored in directory: /root/.cache/pip/wheels/fc/89/43/f2f5bc00b03ef9724b0f6254a97eaf159a4c4ddc024b33e07a\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b68de206f379e2029bd72f9ee30c66d5dde34d7b96a6ea74a24dabbb92fafe7d\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sentence-transformers sacremoses\n","Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, sentence-transformers\n","Successfully installed sacremoses-0.0.43 sentence-transformers-0.3.9 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 245M/245M [00:33<00:00, 7.22MB/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fVN6Lt7Nudbr"},"source":["def generateEmbeddings(reference):\n","\n","  authors_list = reference[\"authors\"]\n","  title = reference[\"title\"]\n","  venue = reference[\"venue\"]\n","  reference_embedding = {}\n","\n","  for i in range(len(authors_list)):\n","    main_author_embeddings = []\n","    main_author = authors_list[i]\n","    co_authors = [auth for auth in authors_list if auth != main_author] \n","\n","    main_author_emb = chars2vec_model.vectorize_words([main_author])[0]\n","    title_emb = bert_model.encode(title)\n","    venue_emb = bert_model.encode(venue_fullforms[venue])\n","\n","    for co_author in co_authors:\n","       co_author_emb = chars2vec_model.vectorize_words([co_author])[0]\n","       main_author_embeddings.append(np.concatenate([main_author_emb,co_author_emb,title_emb,venue_emb]))\n","    main_author_embeddings = np.array(main_author_embeddings)\n","    reference_embedding[i] = main_author_embeddings\n","  \n","  return reference_embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBzPYfaEus15"},"source":["def predict_authors(reference,saved_model):\n","\n","  predicted_authors_dict = {}\n","  reference_embedding = generateEmbeddings(reference)\n","  for main_auth_index in reference_embedding:\n","\n","    main_author_embeddings = reference_embedding[main_auth_index]\n","    main_author_predictions = []\n","\n","    for i in range(len(main_author_embeddings)):\n","      emb = main_author_embeddings[i]\n","      emb = tf.keras.utils.normalize(emb)\n","      main_author_predictions.append(saved_model.predict(emb)[0])\n","\n","    main_author_predictions = np.array(main_author_predictions)\n","    main_author_predictions_sum = np.sum(main_author_predictions,axis=0)\n","    index_of_main_author = np.argmax(main_author_predictions_sum)\n","    predicted_authors_dict[reference[\"authors\"][main_auth_index]] = index2auth[index_of_main_author]\n","    \n","  return predicted_authors_dict\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3JTX-2iu5u3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606463123129,"user_tz":-60,"elapsed":1074,"user":{"displayName":"nagaraj Asundi","photoUrl":"","userId":"14146095082475060735"}},"outputId":"888f4bda-cd5a-4259-e6c2-630a4524752a"},"source":["ref = {}\n","ref[\"authors\"] = [\"B Li\", \"J Liu\"]\n","ref[\"title\"] = \"Research on Semantic-Based Web Services Registry Federation\"\n","ref[\"venue\"] = \"GCC\"\n","\n","path_to_trained_model = 'data/Reference2Auth_model.h5'\n","saved_model = tf.keras.models.load_model(path_to_trained_model)\n","\n","\n","predicted_list = predict_authors(ref,saved_model)\n","print(predicted_list)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'B Li': 'Bing Li 0010', 'J Liu': 'Jin Liu'}\n"],"name":"stdout"}]}]}